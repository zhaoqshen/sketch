## Redis数据结构

![image-20210416113204169](https://wsx666-1302523054.cos.ap-nanjing.myqcloud.com/image/2021-04-16/image-20210416113204169%20.png)

### hash

+ 为了实现从键到值的快速访问，Redis 使用了一个哈希表来保存所有键值对。
+ 一个哈希表，其实就是一个数组，数组的每个元素称为一个哈希桶
+ 哈希桶中的元素保存的并不是值本身，而是指向具体值的指针。这也就是 说，不管值是 String，还是集合类型，哈希桶中的元素都是指向它们的指针
+ 哈希桶中的 entry 元素中保存了*key和*value指针，分别指向了 实际的键和值，这样一来，即使值是一个集合，也可以通过*value指针被查找到。

![image-20210416114401058](https://wsx666-1302523054.cos.ap-nanjing.myqcloud.com/image/2021-04-16/image-20210416114401058%20.png)

> 因为这个哈希表保存了所有的键值对把。称为全局哈希表。潜在的风险点：是哈希表的冲突问题和 rehash 可能带来的操作阻塞。

​		Redis 解决哈希冲突的方式，就是链式哈希。链式哈希也很容易理解，就是指**同一个哈希 桶中的多个元素用一个链表来保存，它们之间依次用指针连接**。

如下图所示:entry1、entry2 和 entry3 都需要保存在哈希桶 3 中，导致了哈希冲突。此 时，entry1 元素会通过一个*next指针指向 entry2，同样，entry2 也会通过*next指针 指向 entry3。这样一来，即使哈希桶 3 中的元素有 100 个，我们也可以通过 entry 元素 中的指针，把它们连起来。这就形成了一个链表，也叫作哈希冲突链。

![image-20210416114556048](https://wsx666-1302523054.cos.ap-nanjing.myqcloud.com/image/2021-04-16/image-20210416114556048%20.png)

​		Redis 会对哈希表做 rehash 操作。rehash 也就是增加现有的哈希桶数量，让逐渐 增多的 entry 元素能在更多的桶之间分散保存，减少单个桶中的元素数量，从而减少单个 桶中的冲突。

​		Redis 默认使用了两个全局哈希表:哈希表 1 和哈希 表 2。一开始，当你刚插入数据时，默认使用哈希表 1，此时的哈希表 2 并没有被分配空 间。随着数据逐步增多，Redis 开始执行 rehash，这个过程分为三步:

+ 给哈希表 2 分配更大的空间，例如是当前哈希表 1 大小的两倍;
+  把哈希表 1 中的数据重新映射并拷贝到哈希表 2 中;
+ 释放哈希表 1 的空间。

到此，我们就可以从哈希表 1 切换到哈希表 2，用增大的哈希表 2 保存更多数据，而原来 的哈希表 1 留作下一次 rehash 扩容备用。

​		第二步涉及大量的数据拷贝，如果一次性把哈希表 1 中的数据都 迁移完，会造成 Redis 线程阻塞，无法服务其他请求。此时，Redis 就无法快速访问数据 了。为了避免这个问题，Redis 采用了**渐进式 rehash**。

​		到第二步拷贝数据时，Redis 仍然正常处理客户端请求，每处理一个请求 时，从哈希表 1 中的第一个索引位置开始，顺带着将这个索引位置上的所有 entries 拷贝 到哈希表 2 中;等处理下一个请求时，再顺带拷贝哈希表 1 中的下一个索引位置的 entries。

​		![image-20210416114902737](https://wsx666-1302523054.cos.ap-nanjing.myqcloud.com/image/2021-04-16/image-20210416114902737%20.png)

> 这样就巧妙地把一次性大量拷贝的开销，分摊到了多次处理请求的过程中，避免了耗时操作，保证了数据的快速访问。

### 压缩列表

+ 压缩列表实际上类似于一个数组，数组中的每一个元素都对应保存一个数据。和数组不同 的是，压缩列表在表头有三个字段 zlbytes、zltail 和 zllen，分别表示列表长度、列表尾的 偏移量和列表中的 entry 个数;压缩列表在表尾还有一个 zlend，表示列表结束

![image-20210416132038556](https://wsx666-1302523054.cos.ap-nanjing.myqcloud.com/image/2021-04-16/image-20210416132038556%20.png)

​		在压缩列表中，如果我们要查找定位第一个元素和最后一个元素，可以通过表头三个字段 的长度直接定位，复杂度是 O(1)。而查找其他元素时，就没有这么高效了，只能逐个查 找，此时的复杂度就是 O(N) 了。

> quicklist是由ziplist组成的双向链表，链表中的每一个节点都以压缩列表ziplist的结构保存着数据，而ziplist有多个entry节点，保存着数据。相当与一个quicklist节点保存的是**一片数据，而不再是一个数据**。

### 跳表

**有序链表**只能逐一查找元素，导致操作起来非常缓慢，于是就出现了跳表。具体来说，跳 表在链表的基础上，**增加了多级索引，通过索引位置的几个跳转，实现数据的快速定位**

![image-20210416132200789](https://wsx666-1302523054.cos.ap-nanjing.myqcloud.com/image/2021-04-16/image-20210416132200789%20.png)

### 数据结构的时间复杂度

![image-20210416132350618](https://wsx666-1302523054.cos.ap-nanjing.myqcloud.com/image/2021-04-16/image-20210416132350618%20.png)

### 不同操作的复杂度

+ 第一，**单元素操作，是指每一种集合类型对单个数据实现的增删改查操作**。例如，Hash 类 型的 HGET、HSET 和 HDEL，Set 类型的 SADD、SREM、SRANDMEMBER 等。这些操 作的复杂度由集合采用的数据结构决定，例如，HGET、HSET 和 HDEL 是对哈希表做操 作，所以它们的复杂度都是 O(1);Set 类型用哈希表作为底层数据结构时，它的 SADD、 SREM、SRANDMEMBER 复杂度也是 O(1)。

> 集合类型支持同时对多个元素进行增删改查，例如 Hash 类型的 HMGET 和 HMSET，Set 类型的 SADD 也支持同时增加多个元素。此时，这些操 作的复杂度，就是由单个元素操作复杂度和元素个数决定的。例如，HMSET 增加 M 个元 素时，复杂度就从 O(1) 变成 O(M) 了。

+ **范围操作，是指集合类型中的遍历操作，可以返回集合中的所有数据**，比如 Hash 类型的 HGETALL 和 Set 类型的 SMEMBERS，或者返回一个范围内的部分数据，比如 List 类型的 LRANGE 和 ZSet 类型的 ZRANGE。**这类操作的复杂度一般是 O(N)，比较耗时， 我们应该尽量避免**。

> Redis 从 2.8 版本开始提供了 SCAN 系列操作(包括 HSCAN，SSCAN 和 ZSCAN)，这类操作实现了渐进式遍历，每次只返回有限数量的数据。这样一来，相比于 HGETALL、SMEMBERS 这类操作来说，就避免了一次性返回所有元素而导致的 Redis 阻 塞。

+ 第三，统计操作，是指**集合类型对集合中所有元素个数的记录**例如 LLEN 和 SCARD。这 类操作复杂度只有 O(1)，这是因为当集合类型采用压缩列表、双向链表、整数数组这些数 据结构时，这些结构中专门记录了元素的个数统计，因此可以高效地完成相关操作。
+ 第四，例外情况，是指某些数据结构的特殊记录，例如**压缩列表和双向链表都会记录表头 和表尾的偏移量**。这样一来，对于 List 类型的 LPOP、RPOP、LPUSH、RPUSH 这四个操 作来说，它们是在列表的头尾增删元素，这就可以通过偏移量直接定位，所以它们的复杂 度也只有 O(1)，可以实现快速操作。
+ 整数数组和压缩列表在查找时间复杂度方面并没有很大的优势，那为什么 Redis 还会把它 们作为底层数据结构呢?
  + 内存利用率，数组和压缩列表都是非常紧凑的数据结构，它比链表占用的内存要更少。Redis是内存数据库，大量数据存到内存中，此时需要做尽可能的优化，提高内存的利用率。
  + 数组对CPU高速缓存支持更友好，所以Redis在设计时，集合数据元素较少情况下，默认采用内存紧凑排列的方式存储，同时利用CPU高速缓存不会降低访问速度。当数据元素超过设定阈值后，避免查询时间复杂度太高，转为哈希和跳表数据结构存储，保证查询效率。

## 为什么单线程的 Redis 能那么快

+ Redis 是单线程，主要是指 **Redis 的网络 IO 和键值对读写是由一个线程来完成的，这也是 Redis 对外提供键值存储服务的主要流程**。 但 Redis 的其他功能，比如持久化、异步删除、集群数据同步等，其实是由额外的线程执 行的。

+ **多线程编程模式面临的共享资源的 并发访问控制问题**。

+ Redis 网络框架调用 epoll 机制，让内核监听这些套接字。此时，Redis 线程不会阻塞在某一个特定的监听或已连接套接字上，也就是说，不会阻塞在某一个特定的客户端请求处理 上。正因为此，Redis 可以同时和多个客户端连接并处理请求，从而提升并发性。

  ![image-20210416140817223](https://wsx666-1302523054.cos.ap-nanjing.myqcloud.com/image/2021-04-16/image-20210416140817223%20.png)

​      select/epoll 提供了**基于事件的回调机制**，即**针 对不同事件的发生，调用相应的处理函数**。，select/epoll 一旦监测到 FD 上有请求到达时，就 会触发相应的事件。

​      这些事件会被放进一个事件队列，Redis 单线程对该事件队列不断进行处理。这样一来， Redis 无需一直轮询是否有请求实际发生，这就可以避免造成 CPU 资源浪费。同时， Redis 在对事件队列中的事件进行处理时，会调用相应的处理函数，这就实现了基于事件 的回调。因为 Redis 一直在对事件队列进行处理，所以能及时响应客户端请求，提升 Redis 的响应性能。

+ 以连接请求和读数据请求为例这两个请求分别对应 Accept 事件和 Read 事件
  + Redis 分别对这两个事件注册 accept 和 get 回调函数
  + 当 Linux 内核监听到有连接请求或读数据请求时，就会触发 Accept 事件 和 Read 事件，此时，内核就会回调 Redis 相应的 accept 和 get 函数进行处理

> 多路复用机制也是适 用的。因为这个机制的实现有很多种，既有基于 Linux 系统下的 select 和 epoll 实现，也 有基于 FreeBSD 的 kqueue 实现，以及基于 Solaris 的 evport 实现，这样，你可以根据 Redis 实际运行的操作系统，选择相应的多路复用实现。
>
> select poll epoll的区别，select和poll本质上没啥区别，就是文件描述符数量的限制，select根据不同的系统，文件描述符限制为1024或者2048，poll没有数量限制。他两都是把文件描述符集合保存在用户态，每次把集合传入内核态，内核态返回ready的文件描述符。
> epoll是通过epoll_create和epoll_ctl和epoll_await三个系统调用完成的，每当接入一个文件描述符，通过ctl添加到内核维护的红黑树中，通过事件机制，当数据ready后，从红黑树移动到链表，通过await获取链表中准备好数据的fd，程序去处理。

+ Redis单线程处理IO请求性能瓶颈

  + 任意一个请求在server中一旦发生耗时，都会影响整个server的性能，也就是说后面的请求都要等前面这个耗时请求处理完成，自己才能被处理到；
    + 操作bigkey：写入一个bigkey在分配内存时需要消耗更多的时间，同样，删除bigkey释放内存同样会产生耗时；
    + 用复杂度过高的命令：例如SORT/SUNION/ZUNIONSTORE，或者O(N)命令，但是N很大，例如lrange key 0 -1一次查询全量数据；
    + 大量key集中过期：Redis的过期机制也是在主线程中执行的，大量key集中过期会导致处理一个请求时，耗时都在删除过期key，耗时变长
    + 淘汰策略：淘汰策略也是在主线程执行的，当内存超过Redis内存上限后，每次写入都需要淘汰一些key，也会造成耗时变长
    + AOF刷盘开启always机制：每次写入都需要把这个操作刷到磁盘，写磁盘的速度远比写内存慢，会拖慢Redis的性能
    + 主从全量同步生成RDB：虽然采用fork子进程生成数据快照，但fork这一瞬间也是会阻塞整个线程的，实例越大，阻塞时间越久；
    + 并发量非常大时，单线程读写客户端IO数据存在性能瓶颈，虽然采用IO多路复用机制，但是读写客户端数据依旧是同步IO，只能单线程依次读取客户端的数据，无法利用到CPU多核。

  > 针对问题1，一方面需要业务人员去规避，一方面Redis在4.0推出了lazy-free机制，把bigkey释放内存的耗时操作放在了异步线程中执行，降低对主线程的影响。
  >
  > 针对问题2，Redis在6.0推出了多线程，可以在高并发场景下利用CPU多核多线程读写客户端数据，进一步提升server性能，当然，只是针对客户端的读写是并行的，每个命令的真正操作依旧是单线程的。

## AOF

### AOF日志实现

+ 数据库的写前日志(Write Ahead Log, WAL)，也就是 说，在实际写数据前，先把修改的数据记到日志文件中，以便故障时进行恢复

+ AOF 日志正好相反，它是写后日志，“写后”的意思是 Redis 是先执行命令，把数据写入 内存，然后才记录日志

  ![image-20210416142502101](https://wsx666-1302523054.cos.ap-nanjing.myqcloud.com/image/2021-04-16/image-20210416142502101%20.png)

+ 传统数据库的日志，例如 redo log(重做日志)，记录的是修改后的数据，而 AOF 里记 录的是 Redis 收到的每一条命令，这些命令是以文本形式保存的。

+ Redis 收到“set testkey testvalue”命令后记录的日志为例，看看 AOF 日志的内 容。其中，“*3”表示当前命令有三个部分，每部分都是由“$+数字”开头，后面紧跟着 具体的命令、键或值。这里，“数字”表示这部分中的命令、键或值一共有多少字节。例 如，“$3 set”表示这部分有 3 个字节，也就是“set”命令。

  ![image-20210416142626550](https://wsx666-1302523054.cos.ap-nanjing.myqcloud.com/image/2021-04-16/image-20210416142626550%20.png)

+ 为了避免额外的检查开销，Redis 在向 AOF 里面记录日志的时候，并不会先去对这 些命令进行语法检查。所以，如果先记日志再执行命令的话，日志中就有可能记录了错误 的命令，Redis 在使用日志恢复数据时，就可能会出错。
+ 而写后日志这种方式，就是先让系统执行命令，只有命令能执行成功，才会被记录到日志 中，否则，系统就会直接向客户端报错。所以，Redis 使用写后日志这一方式的一大好处 是，可以避免出现记录错误命令的情况。
+ AOF 还有一个好处:它是在命令执行后才记录日志，所以**不会阻塞当前的写操 作**。
+ 如果刚执行完一个命令，还没有来得及记日志就宕机了，那么这个命令和相应的数 据就有丢失的风险。如果此时 Redis 是用作缓存，还可以从后端数据库重新读入数据进行恢复，但是，如果 Redis 是直接用作数据库的话，此时，因为命令没有记入日志，所以就 无法用日志进行恢复了。
+ AOF 虽然避免了对当前命令的阻塞，但可能会给下一个操作带来阻塞风险。这是因 为，AOF 日志也是在主线程中执行的，如果在把日志文件写入磁盘时，磁盘写压力大，就 会导致写盘很慢，进而导致后续的操作也无法执行了。

###  AOF 配置项 appendfsync

+ **Always**，同步写回:每个写命令执行完，立马同步地将日志写回磁盘;

  >  缺点： 同步写回”可以做到基本不丢数据，但是它在每一个写命令后都有一个慢速的落盘操作，不可避免地会影响主线程性能;

+ **Everysec**，每秒写回:每个写命令执行完，只是先把日志写到 AOF 文件的内存缓冲 区，每隔一秒把缓冲区中的内容写入磁盘;

  > “每秒写回”采用一秒写回一次的频率，避免了“同步写回”的性能开销，虽然减少了对系统性能的影响，但是如果发生宕机，上一秒内未落盘的命令操作仍然会丢失。所 以，这只能算是，在避免影响主线程性能和避免数据丢失两者间取了个折中。

+ **No**，操作系统控制的写回:每个写命令执行完，只是先把日志写到 AOF 文件的内存缓 冲区，由操作系统决定何时将缓冲区内容写回磁盘。

  >  缺点：在写完缓冲区后，就可以继续执行后续的命令，但是落盘 的时机已经不在 Redis 手中了，只要 AOF 记录没有写回磁盘，一旦宕机对应的数据就 丢失了;

  ![image-20210416143353298](https://wsx666-1302523054.cos.ap-nanjing.myqcloud.com/image/2021-04-16/image-20210416143353298%20.png) 

### **AOF 重写机制**

+ AOF 重写机制就是在重写时，Redis 根据数据库的现状创建一个新的 AOF 文 件，也就是说，读取数据库中的所有键值对，然后对每一个键值对用一条命令记录它的写 入.旧日志文件中的多条命令，在重写后的新日志中变成了一条命令。
+ AOF 文件是以追加的方式，逐一记录接收到的写命令的。当一个键值对被多条 写命令反复修改时，AOF 文件会记录相应的多条命令。但是，在重写的时候，是根据这个 键值对当前的最新状态，为它生成对应的写入命令。这样一来，一个键值对在重写日志中 只用一条命令就行了，而且，在日志恢复时，只用执行这条命令，就可以直接完成这个键 值对的写入了。
+ 虽然 AOF 重写后，日志文件会缩小，但是，要把整个数据库的最新数据的操作日志 都写回磁盘，仍然是一个非常耗时的过程.

> 有两个配置项在控制AOF重写的触发时机：
> 1. auto-aof-rewrite-min-size: 表示运行AOF重写时文件的最小大小，默认为64MB
> 2. auto-aof-rewrite-percentage: 这个值的计算方法是：当前AOF文件大小和上一次重写后AOF文件大小的差值，再除以上一次重写后AOF文件大小。也就是当前AOF文件比上一次重写后AOF文件的增量大小，和上一次重写后AOF文件大小的比值。AOF文件大小同时超出上面这两个配置项时，会触发AOF重写。

### **AOF** 重写

+ 重写过程是由后台线程 bgrewriteaof 来完成的，这也是为了避免阻塞主线程
  + 一个拷贝”就是指，每次执行重写时，主线程 fork 出后台的 bgrewriteaof 子进程。此 时，fork 会把主线程的内存拷贝一份给 bgrewriteaof 子进程，这里面就包含了数据库的 最新数据。然后，bgrewriteaof 子进程就可以在不影响主线程的情况下，逐一把拷贝的数 据写成操作，记入重写日志。
  + 两处日志：因为主线程未阻塞，仍然可以处理新来的操作。此时，如果有写操作，第一处日志就是指 正在使用的 AOF 日志，Redis 会把这个操作写到它的缓冲区。这样一来，即使宕机了，这 个 AOF 日志的操作仍然是齐全的，可以用于恢复。新的 AOF 重写日志。这个操作也会被写到重写日志的缓冲区。这 样，重写日志也不会丢失最新的操作。等到拷贝数据的所有操作记录重写完成后，重写日 志记录的这些最新操作也会写入新的 AOF 文件，以保证数据库最新状态的记录。此时，我 们就可以用新的 AOF 文件替代旧文件了

![image-20210416143952486](https://wsx666-1302523054.cos.ap-nanjing.myqcloud.com/image/2021-04-16/image-20210416143952486%20.png)

每次 AOF 重写时，Redis 会先执行一个内存拷贝，用于重写;然后，使用两个 日志保证在重写过程中，新写入的数据不会丢失。而且，因为 Redis 采用额外的线程进行 数据重写，所以，这个过程并不会阻塞主线程。

> 问题1，Redis采用fork子进程重写AOF文件时，潜在的阻塞风险包括：fork子进程 和 AOF重写过程中父进程产生写入的场景，下面依次介绍。
>
> a、fork子进程，fork这个瞬间一定是会阻塞主线程的，fork采用操作系统提供的写实复制(Copy On Write)机制，就是为了避免一次性拷贝大量内存数据给子进程造成的长时间阻塞问题，但fork子进程需要拷贝进程必要的数据结构，其中有一项就是拷贝内存页表（虚拟内存和物理内存的映射索引表），这个拷贝过程会消耗大量CPU资源，拷贝完成之前整个进程是会阻塞的，阻塞时间取决于整个实例的内存大小，实例越大，内存页表越大，fork阻塞时间越久。拷贝内存页表完成后，子进程与父进程指向相同的内存地址空间，也就是说此时虽然产生了子进程，但是并没有申请与父进程相同的内存大小。那什么时候父子进程才会真正内存分离呢？“写实复制”顾名思义，就是在写发生时，才真正拷贝内存真正的数据，这个过程中，父进程也可能会产生阻塞的风险，就是下面介绍的场景。
>
> b、fork出的子进程指向与父进程相同的内存地址空间，此时子进程就可以执行AOF重写，把内存中的所有数据写入到AOF文件中。但是此时父进程依旧是会有流量写入的，如果父进程操作的是一个已经存在的key，那么这个时候父进程就会真正拷贝这个key对应的内存数据，申请新的内存空间，这样逐渐地，父子进程内存数据开始分离，父子进程逐渐拥有各自独立的内存空间。因为内存分配是以页为单位进行分配的，默认4k，如果父进程此时操作的是一个bigkey，重新申请大块内存耗时会变长，可能会产阻塞风险。另外，如果操作系统开启了内存大页机制(Huge Page，页面大小2M)，那么父进程申请内存时阻塞的概率将会大大提高，所以在Redis机器上需要关闭Huge Page机制。Redis每次fork生成RDB或AOF重写完成后，都可以在Redis log中看到父进程重新申请了多大的内存空间。
>
> 问题2，AOF重写不复用AOF本身的日志，一个原因是父子进程写同一个文件必然会产生竞争问题，控制竞争就意味着会影响父进程的性能。二是如果AOF重写过程中失败了，那么原本的AOF文件相当于被污染了，无法做恢复使用。所以Redis AOF重写一个新文件，重写失败的话，直接删除这个文件就好了，不会对原先的AOF文件产生影响。等重写完成之后，直接替换旧文件即可。

## RDB

+ Redis 来说，它实现类似照片记录效果的方式，就是把某一时刻的状态以文件的形式写 到磁盘上，也就是快照。这样一来，即使宕机，快照文件也不会丢失，数据的可靠性也就 得到了保证。这个快照文件就称为 RDB 文件，其中，RDB 就是 Redis DataBase 的缩 写。
+ RDB 记录的是某一时刻的数据，并不是操作，所以，在做数据恢复时，我 们可以直接把 RDB 文件读入内存，很快地完成恢复。

+ 全量数据越多，RDB 文件就越大，往磁盘 上写数据的时间开销就越大。

+ Redis 提供了两个命令来生成 RDB 文件，分别是 save 和 bgsave。
  
  + save:在主线程中执行，会导致阻塞;
  
+ bgsave:fork一个子进程，专门用于写入 RDB 文件，避免了主线程的阻塞，这也是 Redis RDB 文件生成的默认配置。
  
    > fork子进程，fork这个瞬间一定是会阻塞主线程的
  
+ 为了快照而暂停写操作，肯定是不能接受的。所以这个时候，Redis 就会借助操作系统提 供的写时复制技术(Copy-On-Write, COW)，在执行快照的同时，正常处理写操作。
+ 简单来说，bgsave 子进程是由主线程 fork 生成的，可以共享主线程的所有内存数据。 bgsave 子进程运行后，开始读取主线程的内存数据，并把它们写入 RDB 文件。如果主线程对这些数据也都是读操作(例如图中的键值对 A)，那么，主线程和 bgsave 子进程相互不影响。但是，如果主线程要修改一块数据(例如图中的键值对 C)， 那么，这块数据就会被复制一份，生成该数据的副本。然后，bgsave 子进程会把这个副本 数据写入 RDB 文件，而在这个过程中，主线程仍然可以直接修改原来的数据。

![image-20210416145602496](https://wsx666-1302523054.cos.ap-nanjing.myqcloud.com/image/2021-04-16/image-20210416145602496%20.png)

虽然 bgsave 执行时不阻塞主线程，但是，**如果频繁地执行全量 快照，也会带来两方面的开销**。

+ 一方面，频繁将全量数据写入磁盘，会给磁盘带来很大压力，多个快照竞争有限的磁盘带宽，前一个快照还没有做完，后一个又开始做了，容易造成恶性循环。
+ bgsave 子进程需要通过 fork 操作从主线程创建出来。虽然，子进程在创建后 不会再阻塞主线程，但是，fork 这个创建过程本身会阻塞主线程，而且主线程的内存越 大，阻塞时间越长。如果频繁 fork 出 bgsave 子进程，这就会频繁阻塞主线程了。

Redis 4.0 中提出了一个**混合使用 AOF 日志和内存快照**的方法。简单来说，内存快照以一 定的频率执行，在两次快照之间，使用 AOF 日志记录这期间的所有命令操作。

快速恢复数据库，也就是只需要把 RDB 文件直接读入内存，这就避免了 AOF 需要顺 序、逐一重新执行操作命令带来的低效性能问题。

redis AOF的重写机制是指令整合，但是在redis4.0以后，redis的 AOF 重写的时候就直接把 RDB 的内容写到 AOF 文件开头，将增量的以指令的方式Append到AOF，这样做的好处是可以结合 RDB 和 AOF 的优点, 快速加载同时避免丢失过多的数据。当然缺点也是有的， AOF 里面的 RDB 部分就是压缩格式不再是 AOF 格式，可读性较差

## **主从库如何实现数据一致**

+ Redis 具有高可靠性

  + 数据 尽量少丢失
    + RDB
    + AOF
  + 服务尽量少中断
    + 增加副本冗余量
>  Redis 提供了主从库模式，以保证数据副本的一致，主从库之间采用的是读写分 离的方式。

 **读操作**:主库、从库都可以接收;
 **写操作**:首先到主库执行，然后，主库将写操作同步给从库。

### 主从之间的第一次同步

![image-20210416151226982](https://wsx666-1302523054.cos.ap-nanjing.myqcloud.com/image/2021-04-16/image-20210416151226982%20.png)  

+ 第一阶段是主从库间建立连接、协商同步的过程，主要是为全量复制做准备。在这一步，从库和主库建立起连接，并告诉主库即将进行同步，主库确认回复后，主从库间就可以开 始同步了。

  + 库给主库发送 psync 命令，表示要进行数据同步，主库根据这个命令的参数 来启动复制。psync 命令包含了**主库的 runID** 和**复制进度 offset** 两个参数。

  + runID，是每个 Redis 实例启动时都会自动生成的一个随机 ID，用来唯一标记这个实 例。当从库和主库第一次复制时，因为不知道主库的 runID，所以将 runID 设 为“?”

  + offset，此时设为 -1，表示第一次复制。

  + 主库收到 psync 命令后，会用 FULLRESYNC 响应命令带上两个参数:主库 runID 和主库 目前的复制进度 offset，返回给从库。从库收到响应后，会记录下这两个参数。

    > FFULLRESYNC 响应表示第一次复制采用的全量复制，也就是说， 主库会把当前所有的数据都复制给从库

+ 在第二阶段，**主库将所有数据同步给从库。从库收到数据后，在本地完成数据加载**。这个 过程依赖于内存快照生成的 RDB 文件。

  + 主库执行 bgsave 命令，生成 RDB 文件，接着将文件发给从库。从库接收到 RDB 文件后，会先清空当前数据库，然后加载 RDB 文件。这是因为从库在通过 replicaof 命令开始和主库同步前，可能保存了其他数据。为了避免之前数据的影响，从库需要先把 当前数据库清空。
  + 请求中的写操作并没有记录到刚刚生成的 RDB 文件 中。为了保证主从库的数据一致性，主库会在内存中用专门的 replication buffer，记录 RDB 文件生成后收到的所有写操作。

+ 第三个阶段，主库会把第二阶段执行过程中新收到的写命令，再发送给从 库。具体的操作是，当主库完成 RDB 文件发送后，就会把此时 replication buffer 中的修 改操作发给从库，从库再重新执行这些操作。这样一来，主从库就实现同步了。

### 主从级联模式分担全量复制时的主库压力

+ 要完成两个耗时的操作:生成 RDB 文件和传输 RDB 文件。

+ 如果从库数量很多，而且都要和主库进行全量复制的话，就会导致主库忙于 fork 子进程生 成 RDB 文件，进行数据全量同步。fork 这个操作会阻塞主线程处理正常请求，从而导致 主库响应应用程序的请求速度变慢

+ **通过“主 - 从 - 从”模式将主库生成 RDB 和传输 RDB 的压力， 以级联的方式分散到从库上**。

+ 部署主从集群的时候，可以手动选择一个从库(比如选择内存资源配置较高的从库)，用于级联其他的从库。然后，我们可以再选择一些从库(例如三分之一的从库)，在这些从库上执行如下命令，让它们和刚才所选的从库，建立起主从关系。

  ![image-20210416152135591](https://wsx666-1302523054.cos.ap-nanjing.myqcloud.com/image/2021-04-16/image-20210416152135591%20.png)

主从库间通过全量复制实现数据同步的过程，以及通过“主 - 从 - 从”模式分担主库压力的方式。那么，一旦主从库完成了全量复制，它们之间就会一 直维护一个网络连接，主库会通过这个连接将后续陆续收到的命令操作再同步给从库，这 个过程也称为**基于长连接的命令传播**，可以避免频繁建立连接的开销。

### 主从库间网络断了怎么办

+  Redis 2.8 之前，如果主从库在命令传播时出现了网络闪断，那么，从库就会和主库重 新进行一次全量复制，开销非常大。

+ Redis 2.8 开始，网络断了之后，主从库会采用增量复制的方式继续同步。听名字大概 就可以猜到它和全量复制的不同:全量复制是同步所有数据，而增量复制只会把主从库网 络断连期间主库收到的命令，同步给从库。

+ 当主从库断连后，主库会把断连期间收到的写操作命令，写入 replication buffer，同时也 会把这些操作命令也写入 repl_backlog_buffer 这个缓冲区。repl_backlog_buffer 是一个环形缓冲区，**主库会记录自己写到的位置，从库则会记录自己 已经读到的位置**。

  + 刚开始的时候，主库和从库的写读位置在一起，这算是它们的起始位置。随着主库不断接 收新的写操作，它在缓冲区中的写位置会逐步偏离起始位置，我们通常用偏移量来衡量这 个偏移距离的大小，对主库来说，对应的偏移量就是 master_repl_offset。主库接收的新 写操作越多，这个值就会越大。

    同样，从库在复制完写操作命令后，它在缓冲区中的读位置也开始逐步偏移刚才的起始位 置，此时，从库已复制的偏移量 slave_repl_offset 也在不断增加。正常情况下，这两个偏 移量基本相等。

  + 主从库的连接恢复之后，从库首先会给主库发送 psync 命令，并把自己当前的 slave_repl_offset 发给主库，主库会判断自己的 master_repl_offset 和 slave_repl_offset 之间的差距。

  > repl_backlog_buffer 是一个环形缓冲区，所以在 缓冲区写满后，主库会继续写入，此时，就会覆盖掉之前写入的操作。**如果从库的读取速 度比较慢，就有可能导致从库还未读取的操作被主库新写的操作覆盖了，这会导致主从库 间的数据不一致**。

调整 **repl_backlog_size** 这个参 数。这个参数和所需的缓冲空间大小有关。缓冲空间的计算公式是:缓冲空间大小 = 主库 写入命令速度 * 操作大小 - 主从库间网络传输命令速度 * 操作大小。在实际应用中，考虑 到可能存在一些突发的请求压力，我们通常需要把这个缓冲空间扩大一倍，即 repl_backlog_size = 缓冲空间大小 * 2，这也就是 repl_backlog_size 的最终值。

### 主从库间的复制不使用 AOF 呢?

主从全量同步使用RDB而不使用AOF的原因：

+ RDB文件内容是经过压缩的二进制数据（不同数据类型数据做了针对性优化），文件很小。而AOF文件记录的是每一次写操作的命令，写操作越多文件会变得很大，其中还包括很多对同一个key的多次冗余操作。在主从全量数据同步时，传输RDB文件可以尽量降低对主库机器网络带宽的消耗，从库在加载RDB文件时，一是文件小，读取整个文件的速度会很快，二是因为RDB文件存储的都是二进制数据，从库直接按照RDB协议解析还原数据即可，速度会非常快，而AOF需要依次重放每个写命令，这个过程会经历冗长的处理逻辑，恢复速度相比RDB会慢得多，所以使用RDB进行主从全量同步的成本最低。

+ 假设要使用AOF做全量同步，意味着必须打开AOF功能，打开AOF就要选择文件刷盘的策略，选择不当会严重影响Redis性能。而RDB只有在需要定时备份和主从全量同步数据时才会触发生成一次快照。而在很多丢失数据不敏感的业务场景，其实是不需要开启AOF的。

另外，需要指出老师文章的错误：“当主从库断连后，主库会把断连期间收到的写操作命令，写入 replication buffer，同时也会把这些操作命令也写入 repl_backlog_buffer 这个缓冲区。”

1、主从库连接都断开了，哪里来replication buffer呢？

2、应该不是“主从库断连后”主库才把写操作写入repl_backlog_buffer，只要有从库存在，这个repl_backlog_buffer就会存在。主库的所有写命令除了传播给从库之外，都会在这个repl_backlog_buffer中记录一份，缓存起来，只有预先缓存了这些命令，当从库断连后，从库重新发送psync $master_runid $offset，主库才能通过$offset在repl_backlog_buffer中找到从库断开的位置，只发送$offset之后的增量数据给从库即可。

+ repl_backlog_buffer和replication buffer
  + repl_backlog_buffer：就是上面我解释到的，它是为了从库断开之后，如何找到主从差异数据而设计的环形缓冲区，从而避免全量同步带来的性能开销。如果从库断开时间太久，repl_backlog_buffer环形缓冲区被主库的写命令覆盖了，那么从库连上主库后只能乖乖地进行一次全量同步，所以repl_backlog_buffer配置尽量大一些，可以降低主从断开后全量同步的概率。而在repl_backlog_buffer中找主从差异的数据后，如何发给从库呢？这就用到了replication buffer。
  + replication buffer：Redis和客户端通信也好，和从库通信也好，Redis都需要给分配一个 内存buffer进行数据交互，客户端是一个client，从库也是一个client，我们每个client连上Redis后，Redis都会分配一个client buffer，所有数据交互都是通过这个buffer进行的：Redis先把数据写到这个buffer中，然后再把buffer中的数据发到client socket中再通过网络发送出去，这样就完成了数据交互。所以主从在增量同步时，从库作为一个client，也会分配一个buffer，只不过这个buffer专门用来传播用户的写命令到从库，保证主从数据一致，我们通常把它叫做replication buffer。
  + 再延伸一下，既然有这个内存buffer存在，那么这个buffer有没有限制呢？如果主从在传播命令时，因为某些原因从库处理得非常慢，那么主库上的这个buffer就会持续增长，消耗大量的内存资源，甚至OOM。所以Redis提供了client-output-buffer-limit参数限制这个buffer的大小，如果超过限制，主库会强制断开这个client的连接，也就是说从库处理慢导致主库内存buffer的积压达到限制后，主库会强制断开从库的连接，此时主从复制会中断，中断后如果从库再次发起复制请求，那么此时可能会导致恶性循环，引发复制风暴，这种情况需要格外注意。

## 哨兵机制

+ 哨兵其实就是一个运行在特殊模式下的 Redis 进程，主从库实例运行的同时，它也在运 行。哨兵主要负责的就是三个任务:监控、选主(选择主库)和通知。
+ 监控是指哨兵进程在运行时，周期性地给所有的主从库发送 PING 命令， 检测它们是否仍然在线运行。如果从库没有在规定时间内响应哨兵的 PING 命令，哨兵就 会把它标记为“下线状态”;同样，如果主库也没有在规定时间内响应哨兵的 PING 命 令，哨兵就会判定主库下线，然后开始**自动切换主库**的流程。
+ 这个流程首先是执行哨兵的第二个任务，选主。主库挂了以后，哨兵就需要从很多个从库里，按照一定的规则选择一个从库实例，把它作为新的主库。这一步完成后，现在的集群里就有了新主库。
+ 然后，哨兵会执行最后一个任务:通知。在执行通知任务时，哨兵会把新主库的连接信息 发给其他从库，让它们执行 replicaof 命令，和新主库建立连接，并进行数据复制。同时， 哨兵会把新主库的连接信息通知给客户端，让它们把请求操作发到新主库上。

![image-20210416153704111](https://wsx666-1302523054.cos.ap-nanjing.myqcloud.com/image/2021-04-16/image-20210416153704111%20.png)

### **主观下线和客观下线**

+ **哨兵进程会使用 PING 命令检测它自己和主、从库的网络连接情况，用来判断实例的状 态**。如果哨兵发现主库或从库对 PING 命令的响应超时了，那么，哨兵就会先把它标记 为“主观下线”。
+ 如果检测的是从库，那么，哨兵简单地把它标记为“主观下线”就行了，因为从库的下线影响一般不太大，集群的对外服务不会间断。
+ 如果检测的是主库，那么，哨兵还不能简单地把它标记为“主观下线”，开启主从切换。因为很有可能存在这么一个情况:那就是哨兵误判了，其实主库并没有故障。可是，一旦启动了主从切换，后续的选主和通知操作都会带来额外的计算和通信开销。

> 误判一般会发生在集群网络压力较大、网络拥塞，或者是主库本身压力较大的情况

### 减少误判

+ **通常会采用多实例组成的集群模式进行部署，这也被称为哨兵集 群**。引入多个哨兵实例一起来判断，就可以避免单个哨兵因为自身网络状况不好，而误判主库下线的情况。同时，多个哨兵的网络同时不稳定的概率较小，由它们一起做决策，误判率也能降低。

> 在判断主库是否下线时，不能由一个哨兵说了算，只有大多数的哨兵实例，都判断主库已经“主观下线”了，主库才会被标记为“客观下线”，这个叫法也是表明主库下线成为一个客观事实了。这个判断原则就是:少数服从多数。同时，这会进一步触发哨兵开始主从
> 切换流程。

![image-20210416154830266](https://wsx666-1302523054.cos.ap-nanjing.myqcloud.com/image/2021-04-16/image-20210416154830266%20.png)

客观下线”的标准就是，当有 N 个哨兵实例时，最好要有 N/2 + 1 个实例判 断主库为“主观下线”，才能最终判定主库为“客观下线”。这样一来，就可以减少误判 的概率，也能避免误判带来的无谓的主从库切换。(当然，有多少个实例做出“主观下 线”的判断才可以，可以由 Redis 管理员自行设定)。

### 哨兵选择新的主库

+ 多个从库 中，先按照**一定的筛选条件**，把不符合条件的从库去掉。然后，我们再按照**一定的规则**， 给剩下的从库逐个打分，将得分最高的从库选为新主库

  ![image-20210416155014188](https://wsx666-1302523054.cos.ap-nanjing.myqcloud.com/image/2021-04-16/image-20210416155014188%20.png)

+ **除了要检查从库的当前在线状态，还要判断它之前的网络连接状态**

  > 使用配置项 down-after-milliseconds * 10。其中，down-after- milliseconds 是我们认定主从库断连的最大连接超时时间。如果在 down-after- milliseconds 毫秒内，主从节点都没有通过网络联系上，我们就可以认为主从节点断连 了。如果发生断连的次数超过了 10 次，就说明这个从库的网络状况不好，不适合作为新主 库。

+ 剩余的从库打分
  + 从库优先级
    + slave-priority 配置项，给不同的从库设置不同优先级
    + 选主时， 哨兵会给优先级高的从库打高分，如果有一个从库优先级最高，那么它就是新主库了。如 果从库的优先级都一样，那么哨兵开始第二轮打分。
  + **从库复制进度**
    + 从库同步时有个命令传播的过程。在这个过程中，主库会用 master_repl_offset 记录当前的最新写操作repl_backlog_buffer 中的位置，而从库会 用 slave_repl_offset 这个值记录当前的复制进度
    + 从库它的 slave_repl_offset 需要最接近 master_repl_offset。如果 在所有从库中，有从库的 slave_repl_offset 最接近 master_repl_offset，那么它的得分就 最高，可以作为新主库。
  + **从库 ID 号**
    + **在优先级和复制进度都相同的情况下，ID 号最小的从库得分最 高，会被选为新主库**。

### 哨兵在操作主从切换的过程中，客户端能否正常地进行请求操作

如果客户端使用了读写分离，那么读请求可以在从库上正常执行，不会受到影响。但是由于此时主库已经挂了，而且哨兵还没有选出新的主库，所以在这期间写请求会失败，失败持续的时间 = 哨兵切换主从的时间 + 客户端感知到新主库 的时间。

如果不想让业务感知到异常，客户端只能把写失败的请求先缓存起来或写入消息队列中间件中，等哨兵切换完主从后，再把这些写请求发给新的主库，但这种场景只适合对写入请求返回值不敏感的业务，而且还需要业务层做适配，另外主从切换时间过长，也会导致客户端或消息队列中间件缓存写请求过多，切换完成之后重放这些请求的时间变长。

哨兵检测主库多久没有响应就提升从库为新的主库，这个时间是可以配置的（down-after-milliseconds参数）。配置的时间越短，哨兵越敏感，哨兵集群认为主库在短时间内连不上就会发起主从切换，这种配置很可能因为网络拥塞但主库正常而发生不必要的切换，当然，当主库真正故障时，因为切换得及时，对业务的影响最小。如果配置的时间比较长，哨兵越保守，这种情况可以减少哨兵误判的概率，但是主库故障发生时，业务写失败的时间也会比较久，缓存写请求数据量越多。

应用程序不感知服务的中断，还需要哨兵和客户端做些什么？当哨兵完成主从切换后，客户端需要及时感知到主库发生了变更，然后把缓存的写请求写入到新库中，保证后续写请求不会再受到影响，具体做法如下：

哨兵提升一个从库为新主库后，哨兵会把新主库的地址写入自己实例的pubsub（switch-master）中。客户端需要订阅这个pubsub，当这个pubsub有数据时，客户端就能感知到主库发生变更，同时可以拿到最新的主库地址，然后把写请求写到这个新主库即可，这种机制属于哨兵主动通知客户端。

如果客户端因为某些原因错过了哨兵的通知，或者哨兵通知后客户端处理失败了，安全起见，客户端也需要支持主动去获取最新主从的地址进行访问。

所以，客户端需要访问主从库时，不能直接写死主从库的地址了，而是需要从哨兵集群中获取最新的地址（sentinel get-master-addr-by-name命令），这样当实例异常时，哨兵切换后或者客户端断开重连，都可以从哨兵集群中拿到最新的实例地址。

一般Redis的SDK都提供了通过哨兵拿到实例地址，再访问实例的方式，我们直接使用即可，不需要自己实现这些逻辑。当然，对于只有主从实例的情况，客户端需要和哨兵配合使用，而在分片集群模式下，这些逻辑都可以做在proxy层，这样客户端也不需要关心这些逻辑了，Codis就是这么做的。

### 哨兵集群中有实例挂了，怎么办，会影响主库状态判断和选主吗？

这个属于分布式系统领域的问题了，指的是在分布式系统中，如果存在故障节点，整个集群是否还可以提供服务？而且提供的服务是正确的？

这是一个分布式系统容错问题，这方面最著名的就是分布式领域中的“拜占庭将军”问题了，“拜占庭将军问题”不仅解决了容错问题，还可以解决错误节点的问题，虽然比较复杂，但还是值得研究的，有兴趣的同学可以去了解下。

简单说结论：存在故障节点时，只要集群中大多数节点状态正常，集群依旧可以对外提供服务。具体推导过程细节很多，大家去查前面的资料了解就好。

### 哨兵集群多数实例达成共识，判断出主库“客观下线”后，由哪个实例来执行主从切换呢？

哨兵集群判断出主库“主观下线”后，会选出一个“哨兵领导者”，之后整个过程由它来完成主从切换。

但是如何选出“哨兵领导者”？这个问题也是一个分布式系统中的问题，就是我们经常听说的共识算法，指的是集群中多个节点如何就一个问题达成共识。共识算法有很多种，例如Paxos、Raft，这里哨兵集群采用的类似于Raft的共识算法。

简单来说就是每个哨兵设置一个随机超时时间，超时后每个哨兵会请求其他哨兵为自己投票，其他哨兵节点对收到的第一个请求进行投票确认，一轮投票下来后，首先达到多数选票的哨兵节点成为“哨兵领导者”，如果没有达到多数选票的哨兵节点，那么会重新选举，直到能够成功选出“哨兵领导者”。

## 哨兵集群

+ 哨兵实例之间可以相互发现，要归功于 Redis 提供的 pub/sub 机制，也就是发布 / 订阅机制

+ 哨兵只要和主库建立起了连接，就可以在主库上发布消息了，比如说发布它自己的连接信 息(IP 和端口)。同时，它也可以从主库上订阅消息，获得其他哨兵发布的连接信息。当 多个哨兵实例都在主库上做了发布和订阅操作后，它们之间就能知道彼此的 IP 地址和端 口。

+ **只有订阅了同一个频道的应用，才能通过发布的消息进行信息 交换**。

+ 哨兵 1 把自己的 IP和端口发布到指定频道上其他哨兵订阅了该频道其他哨兵就可以从这个频道直接获取哨兵 1 的 IP 地址和端口号。

+ 哨兵 2、3 可以和哨兵 1 建立网络连接。通过这个方式，哨兵 2 和 3 也可以建立网 络连接，这样一来，哨兵集群就形成了。它们相互间可以通过网络连接进行通信

  ![image-20210416162508551](https://wsx666-1302523054.cos.ap-nanjing.myqcloud.com/image/2021-04-16/image-20210416162508551%20.png)

> 由哨兵向主库发送 INFO 命令来完成的来获取**从库的 IP 地址和端口**。
>
> 哨兵就可以根据从库列 表中的连接信息，和每个从库建立连接，并在这个连接上持续地对从库进行监控

### **基于** **pub/sub** **机制的客户端事件通知**

从本质上说，哨兵就是一个运行在特定模式下的 Redis 实例，只不过它并不服务请求操 作，只是完成监控、选主和通知的任务。所以，每个哨兵实例也提供 pub/sub 机制，客户 端可以从哨兵订阅消息。哨兵提供的消息订阅频道有很多，不同频道包含了主从库切换过 程中的不同关键事件

![image-20210416162822571](https://wsx666-1302523054.cos.ap-nanjing.myqcloud.com/image/2021-04-16/image-20210416162822571%20.png)

+ 所有实例进入客观下线状态的事件 `SUBSCRIBE +odown`

+ 订阅所有的事件:  `PSUBSCRIBE *`

+ 哨兵把新主库选择出来后，客户端就会看到下面的 `switch-master `事件。

  ```c
  switch-master <master name> <oldip> <oldport> <newip> <newport>
  ```


> 客户端就可以用面的新主库地址和端口进行通信

### 由哪个哨兵执行主从切换

+ 确定由哪个哨兵执行主从切换的过程，和主库“客观下线”的判断过程类似，也是一个“投票仲裁”的过程。
+ 客观下线”的原则，接下来，我介绍下具体的判断过程。
  + 任何一个实例只要自身判断主库“主观下线”后，就会给其他实例发送 is-master-down- by-addr 命令。
  + 接着，其他实例会根据自己和主库的连接情况，做出 Y 或 N 的响应，Y 相 当于赞成票，N 相当于反对票。

![image-20210416164434942](https://wsx666-1302523054.cos.ap-nanjing.myqcloud.com/image/2021-04-16/image-20210416164434942%20.png)

​		一个哨兵获得了仲裁所需的赞成票数后，就可以标记主库为“客观下线”。这个所需的赞 成票数是通过哨兵配置文件中的 quorum 配置项设定的。例如，现在有 5 个哨兵， quorum 配置的是 3，那么，一个哨兵需要 3 张赞成票，就可以标记主库为“客观下 线”了。这 3 张赞成票包括哨兵自己的一张赞成票和另外两个哨兵的赞成票。

​		这个哨兵就可以再给其他哨兵发送命令，表明希望由自己来执行主从切换，并让所 有其他哨兵进行投票。这个投票过程称为“Leader 选举”。因为最终执行主从切换的哨兵 称为 Leader，投票过程就是确定 Leader。

​		任何一个想成为 Leader 的哨兵，要满足两个条件:第一，拿到半数以上的 赞成票;第二，拿到的票数同时还需要大于等于哨兵配置文件中的 quorum 值。以 3 个哨 兵为例，假设此时的 quorum 设置为 2，那么，任何一个想成为 Leader 的哨兵只要拿到 2 张赞成票，就可以了。

展示一下 3 个哨兵、quorum 为 2 的选举 过程

![image-20210416164651531](https://wsx666-1302523054.cos.ap-nanjing.myqcloud.com/image/2021-04-16/image-20210416164651531%20.png)

+ 在 T1 时刻，S1 判断主库为“客观下线”，它想成为 Leader，就先给自己投一张赞成票， 然后分别向 S2 和 S3 发送命令，表示要成为 Leader

+ 在 T2 时刻，S3 判断主库为“客观下线”，它也想成为 Leader，所以也先给自己投一张赞 成票，再分别向 S1 和 S2 发送命令，表示要成为 Leader。

+ 在 T3 时刻，S1 收到了 S3 的 Leader 投票请求。因为 S1 已经给自己投了一票 Y，所以它 不能再给其他哨兵投赞成票了，所以 S1 回复 N 表示不同意。同时，S2 收到了 T2 时 S3 发送的 Leader 投票请求。因为 S2 之前没有投过票，它会给第一个向它发送投票请求的哨 兵回复 Y，给后续再发送投票请求的哨兵回复 N，所以，在 T3 时，S2 回复 S3，同意 S3 成为 Leader。

+ 在 T4 时刻，S2 才收到 T1 时 S1 发送的投票命令。因为 S2 已经在 T3 时同意了 S3 的投 票请求，此时，S2 给 S1 回复 N，表示不同意 S1 成为 Leader。发生这种情况，是因为 S3 和 S2 之间的网络传输正常，而 S1 和 S2 之间的网络传输可能正好拥塞了，导致投票请 求传输慢了。

+ 最后，在 T5 时刻，S1 得到的票数是来自它自己的一票 Y 和来自 S2 的一票 N。而 S3 除 了自己的赞成票 Y 以外，还收到了来自 S2 的一票 Y。此时，S3 不仅获得了半数以上的 Leader 赞成票，也达到预设的 quorum 值(quorum 为 2)，所以它最终成为了 Leader。接着，S3 会开始执行选主操作，而且在选定新主库后，会给其他从库和客户端通 知新主库的信息。

+  S3 没有拿到 2 票 Y，那么这轮投票就不会产生 Leader。哨兵集群会等待一段时间 (也就是哨兵故障转移超时时间的 2 倍)，再重新选举。这是因为，哨兵集群能够进行成 功投票，很大程度上依赖于选举命令的正常网络传播。如果网络压力较大或有短时堵塞， 就可能导致没有一个哨兵能拿到半数以上的赞成票。所以，等到网络拥塞好转之后，再进 行投票选举，成功的概率就会增加。

  ### 假设有一个 Redis 集群，是“一主四从”，同时配置了包含 5 个哨兵实例的集群， quorum 值设为 2。在运行过程中，如果有 3 个哨兵实例都发生故障了，此时，Redis 主 库如果有故障，还能正确地判断主库“客观下线”吗?如果可以的话，还能进行主从库自 动切换吗?此外，哨兵实例是不是越多越好呢，如果同时调大 down-after-milliseconds 值，对减少误判是不是也有好处呢?

  Redis 1主4从，5个哨兵，哨兵配置quorum为2，如果3个哨兵故障，当主库宕机时，哨兵能否判断主库“客观下线”？能否自动切换？

  经过实际测试，我的结论如下：

  + 哨兵集群可以判定主库“主观下线”。由于quorum=2，所以当一个哨兵判断主库“主观下线”后，询问另外一个哨兵后也会得到同样的结果，2个哨兵都判定“主观下线”，达到了quorum的值，因此，哨兵集群可以判定主库为“客观下线”。

  + 但哨兵不能完成主从切换。哨兵标记主库“客观下线后”，在选举“哨兵领导者”时，一个哨兵必须拿到超过多数的选票(5/2+1=3票)。但目前只有2个哨兵活着，无论怎么投票，一个哨兵最多只能拿到2票，永远无法达到多数选票的结果。

  但是投票选举过程的细节并不是大家认为的：每个哨兵各自1票，这个情况是不一定的。下面具体说一下：

  场景a：哨兵A先判定主库“主观下线”，然后马上询问哨兵B（注意，此时哨兵B只是被动接受询问，并没有去询问哨兵A，也就是它还没有进入判定“客观下线”的流程），哨兵B回复主库已“主观下线”，达到quorum=2后哨兵A此时可以判定主库“客观下线”。此时，哨兵A马上可以向其他哨兵发起成为“哨兵领导者”的投票，哨兵B收到投票请求后，由于自己还没有询问哨兵A进入判定“客观下线”的流程，所以哨兵B是可以给哨兵A投票确认的，这样哨兵A就已经拿到2票了。等稍后哨兵B也判定“主观下线”后想成为领导者时，因为它已经给别人投过票了，所以这一轮自己就不能再成为领导者了。

  场景b：哨兵A和哨兵B同时判定主库“主观下线”，然后同时询问对方后都得到可以“客观下线”的结论，此时它们各自给自己投上1票后，然后向其他哨兵发起投票请求，但是因为各自都给自己投过票了，因此各自都拒绝了对方的投票请求，这样2个哨兵各自持有1票。

  场景a是1个哨兵拿到2票，场景b是2个哨兵各自有1票，这2种情况都不满足大多数选票(3票)的结果，因此无法完成主从切换。

  经过测试发现，场景b发生的概率非常小，只有2个哨兵同时进入判定“主观下线”的流程时才可以发生。我测试几次后发现，都是复现的场景a。

  哨兵实例是不是越多越好？

  并不是，我们也看到了，哨兵在判定“主观下线”和选举“哨兵领导者”时，都需要和其他节点进行通信，交换信息，哨兵实例越多，通信的次数也就越多，而且部署多个哨兵时，会分布在不同机器上，节点越多带来的机器故障风险也会越大，这些问题都会影响到哨兵的通信和选举，出问题时也就意味着选举时间会变长，切换主从的时间变久。

  调大down-after-milliseconds值，对减少误判是不是有好处？

  是有好处的，适当调大down-after-milliseconds值，当哨兵与主库之间网络存在短时波动时，可以降低误判的概率。但是调大down-after-milliseconds值也意味着主从切换的时间会变长，对业务的影响时间越久，我们需要根据实际场景进行权衡，设置合理的阈值。

  ## **切片集群**

  > INFO 命 令查看 Redis 的 latest_fork_usec 指标值(表示最近一次 fork 的耗时).
  >
  > 而 fork 在执行时会阻塞主线程。数 据量越大，fork 操作造成的主线程阻塞的时间越长。

  切片集群，也叫分片集群，就是指启动多个 Redis 实例组成一个集群，然后按照一定的规 则，把收到的数据划分成多份，每一份用一个实例来保存

  ![image-20210416170149246](https://wsx666-1302523054.cos.ap-nanjing.myqcloud.com/image/2021-04-16/image-20210416170149246%20.png)

### **如何保存更多数据**

+ **纵向扩展**:升级单个 Redis 实例的资源配置，包括增加内存容量、增加磁盘容量、使用 更高配置的 CPU。

  + 优点：**实施起来简单、直接**
  + 潜在风险：当使用 RDB 对数据进行持久化时，如果数据量增加，需要的内存也会增 加，主线程 fork 子进程时就可能会阻塞

+ **横向扩展**:横向增加当前 Redis 实例的个数，

  + **纵向扩展会受到硬件和成本的限制**

### 数据切片和实例的对应分布关系

+  3.0 开始，官方提供了一个 名为 Redis Cluster 的方案，用于实现切片集群
+ Redis Cluster 方案采用哈希槽(Hash Slot，接下来我会直接称之为 Slot)， 来处理数据和实例之间的映射关系。在 Redis Cluster 方案中，一个切片集群共有 16384 个哈希槽，这些哈希槽类似于数据分区，每个键值对都会根据它的 key，被映射到一个哈 希槽中。
  + 首先根据键值对的 key，按照CRC16 算法计算一个 16 bit 的值;
  + 再用这个 16bit 值对 16384 取模，得到 0~16383 范围内的模数，每个模数代表一个相应编号的哈希槽。
+ Redis Cluster 方案时，可以使用 cluster create 命令创建集群，此时，Redis 会自动把这些槽平均分布在集群实例上。例如，如果集群中有 N 个实例，那么，每个实例 上的槽个数为 16384/N 个。
+ cluster meet 命令手动建立实例间的连接，形成集群，再使用 cluster addslots 命令，指定每个实例上的哈希槽个数。

  ![image-20210416170802101](/Users/zhaoqingshen/Library/Application Support/typora-user-images/image-20210416170802101.png)

```c
 redis-cli -h 172.16.19.3 –p 6379 cluster addslots 0,1  
 redis-cli -h 172.16.19.4 –p 6379 cluster addslots 2,3 
 redis-cli -h 172.16.19.5 –p 6379 cluster addslots 4
```

> **在手动分配哈希槽时，需要把 16384 个槽都分配完，否则 Redis 集群无法正常工作**。

### 客户端定位数据

+ 客户端和集群实例建立连接后，实例就会把哈希槽的分配信息发给客户端

+ Redis 实例会把自己的哈希槽信息发给和它相连接的其它实例，来完成哈希槽分配信 息的扩散。

+ 当实例之间相互连接后，每个实例就有所有哈希槽的映射关系了。

+ 客户端收到哈希槽信息后，会把哈希槽信息缓存在本地。当客户端请求键值对时，会先计算键所对应的哈希槽，然后就可以给相应的实例发送请求了

+ 实例和哈希槽的对应关系最常见的变化的两个原因

  + 在集群中，实例有新增或删除，Redis 需要重新分配哈希槽
  + 为了负载均衡，Redis 需要把哈希槽在所有实例上重新分布一遍。

+ 缓存的分配信息和最新的分配信息就不一致了

  + Redis Cluster 方案提供了一种**重定向机制，**所谓的“重定向”，就是指，客户端给一个实 例发送数据读写操作时，这个实例上并没有相应的数据，客户端要再给一个新实例发送操 作命令。
  + 当客户端把一个键值对的操作请 求发给一个实例时，如果这个实例上并没有这个键值对映射的哈希槽，那么，这个实例就 会给客户端返回下面的 MOVED 命令响应结果，这个结果中就包含了新实例的访问地址。

     ```c
  GET hello:key
  (error) MOVED 13320 172.16.19.5:6379
     ```

  > 其中，MOVED 命令表示，客户端请求的键值对所在的哈希槽 13320，实际是在 172.16.19.5 这个实例上。通过返回的 MOVED 命令，就相当于把哈希槽所在的新实例的 信息告诉给客户端了。这样一来，客户端就可以直接和 172.16.19.5 连接，并发送操作请 求了。同时还会更新本地缓存,更新Slot 与与实例的关系

  ```c#
   GET hello:key
  (error) ASK 13320 172.16.19.5:6379
  ```

  >  ASK 命令就表示，客户端请求的键值对所在的哈希槽 13320，在 172.16.19.5 这个实例上，但是这个哈希槽正在迁移。此时，客户端需要先给 172.16.19.5 这个实例发送一个 ASKING 命令。这个命令的意思是，让这个实例允许执行客户端接下来 发送的命令。然后，客户端再向这个实例发送 GET 命令，以读取数据。

    ### MOVED  ASK

+ move命令是在数据迁移完毕后被响应，客户端会更新本地缓存。
+  ASK命令是在数据迁移中被响应，不会让客户端更新缓存

### Redis Cluster不采用把key直接映射到实例的方式，而采用哈希槽的方式原因

+ 整个集群存储key的数量是无法预估的，key的数量非常多时，直接记录每个key对应的实例映射关系，这个映射表会非常庞大，这个映射表无论是存储在服务端还是客户端都占用了非常大的内存空间。

+ Redis Cluster采用无中心化的模式（无proxy，客户端与服务端直连），客户端在某个节点访问一个key，如果这个key不在这个节点上，这个节点需要有纠正客户端路由到正确节点的能力（MOVED响应），这就需要节点之间互相交换路由表，每个节点拥有整个集群完整的路由关系。如果存储的都是key与实例的对应关系，节点之间交换信息也会变得非常庞大，消耗过多的网络资源，而且就算交换完成，相当于每个节点都需要额外存储其他节点的路由表，内存占用过大造成资源浪费。

+ 当集群在扩容、缩容、数据均衡时，节点之间会发生数据迁移，迁移时需要修改每个key的映射关系，维护成本高。 
+ 而在中间增加一层哈希槽，可以把数据和节点解耦，key通过Hash计算，只需要关心映射到了哪个哈希槽，然后再通过哈希槽和节点的映射表找到节点，相当于消耗了很少的CPU资源，不但让数据分布更均匀，还可以让这个映射表变得很小，利于客户端和服务端保存，节点之间交换信息时也变得轻量。

+ 当集群在扩容、缩容、数据均衡时，节点之间的操作例如数据迁移，都以哈希槽为基本单位进行操作，简化了节点扩容、缩容的难度，便于集群的维护和管理。

### 补充一下Redis集群相关的知识

Redis使用集群方案就是为了解决单个节点数据量大、写入量大产生的性能瓶颈的问题。多个节点组成一个集群，可以提高集群的性能和可靠性，但随之而来的就是集群的管理问题，最核心问题有2个：请求路由、数据迁移（扩容/缩容/数据平衡）。

1、请求路由：一般都是采用哈希槽的映射关系表找到指定节点，然后在这个节点上操作的方案。

Redis Cluster在每个节点记录完整的映射关系(便于纠正客户端的错误路由请求)，同时也发给客户端让客户端缓存一份，便于客户端直接找到指定节点，客户端与服务端配合完成数据的路由，这需要业务在使用Redis Cluster时，必须升级为集群版的SDK才支持客户端和服务端的协议交互。

其他Redis集群化方案例如Twemproxy、Codis都是中心化模式（增加Proxy层），客户端通过Proxy对整个集群进行操作，Proxy后面可以挂N多个Redis实例，Proxy层维护了路由的转发逻辑。操作Proxy就像是操作一个普通Redis一样，客户端也不需要更换SDK，而Redis Cluster是把这些路由逻辑做在了SDK中。当然，增加一层Proxy也会带来一定的性能损耗。

2、数据迁移：当集群节点不足以支撑业务需求时，就需要扩容节点，扩容就意味着节点之间的数据需要做迁移，而迁移过程中是否会影响到业务，这也是判定一个集群方案是否成熟的标准。

Twemproxy不支持在线扩容，它只解决了请求路由的问题，扩容时需要停机做数据重新分配。而Redis Cluster和Codis都做到了在线扩容（不影响业务或对业务的影响非常小），重点就是在数据迁移过程中，客户端对于正在迁移的key进行操作时，集群如何处理？还要保证响应正确的结果？

Redis Cluster和Codis都需要服务端和客户端/Proxy层互相配合，迁移过程中，服务端针对正在迁移的key，需要让客户端或Proxy去新节点访问（重定向），这个过程就是为了保证业务在访问这些key时依旧不受影响，而且可以得到正确的结果。由于重定向的存在，所以这个期间的访问延迟会变大。等迁移完成之后，Redis Cluster每个节点会更新路由映射表，同时也会让客户端感知到，更新客户端缓存。Codis会在Proxy层更新路由表，客户端在整个过程中无感知。

除了访问正确的节点之外，数据迁移过程中还需要解决异常情况（迁移超时、迁移失败）、性能问题（如何让数据迁移更快、bigkey如何处理），这个过程中的细节也很多。

Redis Cluster的数据迁移是同步的，迁移一个key会同时阻塞源节点和目标节点，迁移过程中会有性能问题。而Codis提供了异步迁移数据的方案，迁移速度更快，对性能影响最小，当然，实现方案也比较复杂。

## **String**

+ String 类型并不是适用于所有场合的，它有 一个明显的短板，就是它保存数据时所消耗的内存空间较多

+ String 类型还需要额外的内存空间记录数据长度、空间使用等 信息，这些信息也叫作元数据。当实际保存的数据较小时，元数据的空间开销就显得比较 大了

+ 当保存 64 位有符号整数时，String 类型会把它保存为一个 8 字节的 Long 类型整数， 这种保存方式通常也叫作 int 编码方式

+ 当保存的数据中包含字符时，String 类型就会用简单动态字符串(Simple Dynamic String，SDS)结构体来保存

  ![image-20210416180554076](https://wsx666-1302523054.cos.ap-nanjing.myqcloud.com/image/2021-04-16/image-20210416180554076%20.png)

+ **buf**:字节数组，保存实际数据。为了表示字节数组的结束，Redis 会自动在数组最后加 一个“\0”，这就会额外占用 1 个字节的开销。
+ **len**:占 4 个字节，表示 buf 的已用长度
+ **alloc**:也占个 4 字节，表示 buf 的实际分配长度，一般大于 len。

在 SDS 中，buf 保存实际数据，而 len 和 alloc 本身其实是 SDS 结构体的额外 开销。

对于 String 类型来说，除了 SDS 的额外开销，还有一个来自于 RedisObject 结构 体的开销。

> 一个 RedisObject 包含了 8 字节的元数据和一个 8 字节指针，这个指针再进一步指向具体 数据类型的实际数据所在。

![image-20210420155236436](https://wsx666-1302523054.cos.ap-nanjing.myqcloud.com/image/2021-04-20/image-20210420155236436%20.png)

+ 当保存的是 Long 类型整数时，RedisObject 中的指针就直接赋值为整数数据 了，这样就不用额外的指针再指向整数了，节省了指针的空间开销。
+ 当保存的是字符串数据，并且字符串小于等于 44 字节时，RedisObject 中的元 数据、指针和 SDS 是一块连续的内存区域，这样就可以避免内存碎片。这种布局方式也被 称为 embstr 编码方式。
+ 当字符串大于 44 字节时，SDS 的数据量就开始变多了，Redis 就不再把 SDS 和 RedisObject 布局在一起了，而是会给 SDS 分配独立的空间，并用指针指向 SDS 结构。 这种布局方式被称为 raw 编码模式。

![image-20210420155344892](https://wsx666-1302523054.cos.ap-nanjing.myqcloud.com/image/2021-04-20/image-20210420155344892%20.png)

​	Redis 会使用一个全局哈希表保存所有键值对，哈希表的每一项是 一个 dictEntry 的结构体，用来指向一个键值对。dictEntry 结构中有三个 8 字节的指针， 分别指向 key、value 以及下一个 dictEntry，三个指针共 24 字节

![image-20210420155443986](https://wsx666-1302523054.cos.ap-nanjing.myqcloud.com/image/2021-04-20/image-20210420155443986%20.png)

​	 Redis 使用的内 存分配库 jemalloc 。jemalloc 在分配内存时，会根据我们申请的字节数 N，找一个比 N 大，但是最接近 N 的 2 的幂次数作为分配的空间，这样可以减少频繁分配的次数。

	### ziplist

+ 表头有三个字段 zlbytes、zltail 和 zllen，分别表示列表长 度、列表尾的偏移量，以及列表中的 entry 个数。压缩列表尾还有一个 zlend，表示列表 结束。

![image-20210420161306415](https://wsx666-1302523054.cos.ap-nanjing.myqcloud.com/image/2021-04-20/image-20210420161306415%20.png)

> 压缩列表之所以能节省内存，就在于它是用一系列连续的 entry 保存数据

+  entry 的 元数据
  + **prev_len** 表示前一个 entry 的长度 当上 一个 entry 长度小于 254 字节时，prev_len 取值为 1 字节，否则，就取值为 5 字节
  + **len** 表示自身长度，4 字节
  + **encoding **表示编码方式，1 字节;
  + **content** 保存实际数据。

>  String 类型时，一个键值对就有一个 dictEntry， 要用 32 字节空间。但采用集合类型时，一个 key 就对应一个集合的数据，能保存的数据 多了很多，但也只用了一个 dictEntry，这样就节省了内存

### hash

Redis Hash 类型的两种底层实现结构，分别是压缩列表和哈希 表。

+ hash-max-ziplist-entries:表示用压缩列表保存时哈希集合中的最大元素个数
+ hash-max-ziplist-value:表示用压缩列表保存时哈希集合中单个元素的最大长度。

> Hash 集合中写入的元素个数超过了 hash-max-ziplist-entries，或者写入的 单个元素大小超过了 hash-max-ziplist-value，Redis 就会自动把 Hash 类型的实现结构 由压缩列表转为哈希表。

## 统计相关

+ 聚合统计： 指统计多个集合元素的聚合结果，包括:统计多个集合的共有元素(交集统计);把两个集合相比，统计其中一个集合独有的元素(差集统计);统计多个集合的所有元素(并集统计)
  + SUNIONSTORE 取并集      SUNIONSTORE  要保存的到的集合  集合1 集合2 ..
  + SDIFFSTORE 取差集  SDIFFSTORE  要保存的到的集合 集合1 集合2 ..
  + SINTERSTORE 取交集  SINTERSTORE     要保存的到的集合  集合1 集合2 ..
+ 排序统计： **List 是按照元素进入 List 的顺序进行排序的，而 Sorted Set 可以根据元素的权重来排 序**
+ **二值状态统计**： 二值状态就是指集合元素的取 值就只有 0 和 1 两种
+ **基数统计**： 基数统计。基数统计就是指统计一个集合中不重复的元 素个数。对应到我们刚才介绍的场景中，就是统计网页的 UV。

![image-20210420162703892](https://wsx666-1302523054.cos.ap-nanjing.myqcloud.com/image/2021-04-20/image-20210420162703892%20.png)

## **面向** **LBS** **应用的** **GEO** **数据类型**

+ 基于位置信息服务(Location-Based Service，LBS)的应用

## Redis做做消息队列的

### List

+ **BRPOP 命令也称为阻塞式读取，客户端 在没有读到队列数据时，自动阻塞，直到有新的数据写入队列，再开始读取新数据**
+ List 类型提供了 BRPOPLPUSH 命令，这个命令的作用是让消费者程序从 一个 List 中读取消息，同时，Redis 会把这个消息再插入到另一个 List(可以叫作备份 List)留存。

### **基于** **Streams** 的消息队列解决方案
+ XADD:插入消息，保证有序，可以自动生成全局唯一 ID;
  + XADD 命令可以往消息队列中插入新消息，消息的格式是键 - 值对形式。对于插入的每一 条消息，Streams 可以自动为其生成一个全局唯一的 ID。

+ XREAD:用于读取消息，可以按 ID 读取数据;

+ XREADGROUP:按消费组形式读取消息;

+ XPENDING 和 XACK:XPENDING 命令可以用来查询每个消费组内所有消费者已读取 但尚未确认的消息，而 XACK 命令用于向消息队列确认消息处理已完成。

![image-20210420170048432](https://wsx666-1302523054.cos.ap-nanjing.myqcloud.com/image/2021-04-20/image-20210420170048432%20.png)

## Redis 的阻塞点

![image-20210420170241139](https://wsx666-1302523054.cos.ap-nanjing.myqcloud.com/image/2021-04-20/image-20210420170241139%20.png)

>  删除操作的本质是要释放键值对占用的内存空间。释放内存只是第一步，为了更加高效地管理内存空间，在应用程序释放内存时，操作系统 需要把释放掉的内存块插入一个空闲内存块的链表，以便后续进行管理和再分配。这个过 程本身需要一定时间，而且会阻塞当前释放内存的应用程序，所以，如果一下子释放了大 量内存，空闲内存块链表操作时间就会增加，相应地就会造成 Redis 主线程的阻塞。

​		Redis 性能受损的 5 大阻塞 点，包括集合全量查询和聚合操作、bigkey 删除、清空数据库、AOF 日志同步写，以及 从库加载 RDB 文件。

​		bigkey 删除、清空数据库、AOF 日志同步写不属于关键路径操作， 可以使用异步子线程机制来完成。Redis 在运行时会创建三个子线程，主线程会通过一个 任务队列和三个子线程进行交互。子线程会根据任务的具体类型，来执行相应的异步操 作。

​		先使用集合类型提供的 SCAN 命令读取数据， 然后再进行删除。因为用 SCAN 命令可以每次只读取一部分数据并进行删除，这样可以避 免一次性删除大量 key 给主线程带来的阻塞。

​		集合全量查询和聚合操作:可以使用 SCAN 命令，分批读取数据，再在客户端进行聚合 计算;

​		从库加载 RDB 文件:把主库的数据量大小控制在 2~4GB 左右，以保证 RDB 文件能以 较快的速度加载。

​		如果一个操作能被异步执行，就意味着，它并不是 Redis 主线程的关键路径上的操作。

​		对于 Redis 来说，**读操作是典型的关键路径操作**，因为客户端发送了读操作之后，就会等 待读取的数据返回，以便进行后续的数据处理。而 Redis 的第一个阻塞点“集合全量查询 和聚合操作”都涉及到了读操作，所以，它们是不能进行异步操作了。

​		删除操作并不需要给客户端返回具体的数据结果，所以不算是关 键路径操作。而我们刚才总结的第二个阻塞点“bigkey 删除”，和第三个阻塞点“清空数 据库”，都是对数据做删除，并不在关键路径上。因此，我们可以使用后台子线程来异步 执行删除操作。

​		第四个阻塞点“AOF 日志同步写”来说，为了保证数据可靠性，Redis 实例需要保证 AOF 日志中的操作记录已经落盘，这个操作虽然需要实例等待，但它并不会返回具体的数 据结果给实例

​		从库加载 RDB 文件”这个阻塞点。从库要想对客户端提供数据存取 服务，就必须把 RDB 文件加载完成。所以，这个操作也属于关键路径上的操作，我们必须 让从库的主线程来执行。

​		Redis 主线程启动后，会使用操作系统提供的 pthread_create 函数创建 3 个子线程，分别 由它们负责 AOF 日志写操作、键值对删除以及文件关闭的异步执行。

​		主线程通过一个链表形式的任务队列和子线程进行交互。当收到键值对删除和清空数据库的操作时，主线程会把这个操作封装成一个任务，放入到任务队列中，然后给客户端返回一个完成信息，表明删除已经完成。

​		但实际上，这个时候删除还没有执行，等到后台子线程从任务队列中读取任务后，才开始 实际删除键值对，并释放相应的内存空间。因此，我们把这种异步删除也称为惰性删除 (lazy free)。

​		AOF 日志配置成 everysec 选项后，主线程会把 AOF 写日志操作封 装成一个任务，也放到任务队列中。后台子线程读取任务后，开始自行写入 AOF 日志，这 样主线程就不用一直等待 AOF 日志写完了。

​		键值对删除:当你的集合类型中有大量元素(例如有百万级别或千万级别元素)需要删 除时，我建议你使用 UNLINK 命令。

​		清空数据库:可以在 FLUSHDB 和 FLUSHALL 命令后加上 ASYNC 选项，这样就可以让 后台子线程异步地清空数据库。

	## CPU与Redis

​		一个 CPU 处理器中一般有多个运行核心，我们把一个运行核心称为一个物理核，每个物理 核都可以运行应用程序。每个物理核都拥有私有的一级缓存(Level 1 cache，简称 L1 cache)，包括一级指令缓存和一级数据缓存，以及私有的二级缓存(Level 2 cache，简 称 L2 cache)。不同的物理核还会共享一个共同的三级缓存(Level 3 cache，简称为 L3 cache)。

​	在多核 CPU 架构下，Redis 如果在不同的核上运行，就需要频繁地进行上下文切换，这个 过程会增加 Redis 的执行时间，客户端也会观察到较高的尾延迟了。所以，建议你在Redis 运行时，把实例和某个核绑定，这样，就能重复利用核上的 L1、L2 缓存，可以降低 响应延迟。

​	为了提升 Redis 的网络性能，我们有时还会把网络中断处理程序和 CPU 核绑定。在这种情 况下，如果服务器使用的是 NUMA 架构，Redis 实例一旦被调度到和中断处理程序不在同 一个 CPU Socket，就要跨 CPU Socket 访问网络数据，这就会降低 Redis 的性能。所 以，我建议你把 Redis 实例和网络中断处理程序绑在同一个 CPU Socket 下的不同核上， 这样可以提升 Redis 的运行性能。

​	当 Redis 实例和 一个逻辑核绑定后，这些子进程和后台线程会和主线程竞争 CPU 资源，也会对 Redis 性能 造成影响。可以把按一个 Redis 实例一个物理核方式进行绑定，这 样，Redis 的主线程、子进程和后台线程可以共享使用一个物理核上的两个逻辑核。

​		当数据删除后，Redis 释放的内存空间会由内存分配器管理，并不会 立即返回给操作系统。

​		Redis 可以使用 libc、jemalloc、tcmalloc 多种内存分配器来分配内存，默认使用 jemalloc。jemalloc 的分配策略之一，是按照一系列固定的大小划分内存空间，例如 8 字节、16 字 节、32 字节、48 字节，..., 2KB、4KB、8KB 等。当程序申请的内存最接近某个固定值 时，jemalloc 会给它分配相应大小的空间。这样的分配方式本身是为了减少分配次数。

​		键值对会被修改和删除，这会导致空间的扩容和释放。

​		mem_fragmentation_ratio 的指标，它表示的就是 Redis 当前的内存碎片 率。

​		used_memory_rss 是操作系统实际分配给 Redis 的物理内存空间，里面就包含了碎片; 而 used_memory 是 Redis 为了保存数据实际申请使用的空间	

​		**mem_fragmentation_ratio 大于 1 但小于 1.5**。这种情况是合理的。

​		**mem_fragmentation_ratio 大于 1.5** 。这表明内存碎片率已经超过了 50%。

​     解决内存碎片

		+ **重启 Redis 实例**
  + 碎片整理
    + Redis 需要启用自动内存碎片清理，可以把 activedefrag 配置项设置为 yes
    + **active-defrag-ignore-bytes 100mb**:表示内存碎片的字节数达到 100MB 时，开始 清理;
    + **active-defrag-threshold-lower 10**:表示内存碎片空间占操作系统分配给 Redis 的 总空间比例达到 10% 时，开始清理。
    + **active-defrag-cycle-min 25**: 表示自动清理过程所用 CPU 时间的比例不低于 25%，保证清理能正常开展;
    + **active-defrag-cycle-max 75**:表示自动清理过程所用 CPU 时间的比例不高于 75%，一旦超过，就停止清理，从而避免在清理时，大量的内存拷贝阻塞 Redis，导致 响应延迟升高。



